{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CIFAR-10 Dataset: 171MB [03:30, 810KB/s]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 12:\n",
      "Image - Min Value: 2 Max Value: 251\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 7 Name: horse\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAFCYoAeAwgQ9ABT2vwF6sxPoFzwxbgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9c35cb02b0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 12\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    norm = x.astype(np.float32)/np.amax(x.astype(np.float32))\n",
    "    return norm\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return np.eye(10)[x]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape=[None, image_shape[0], image_shape[1], image_shape[2]], name=\"x\")\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape=[None, n_classes], name=\"y\")\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    x_tensor_shape = x_tensor.get_shape().as_list()\n",
    "    weights = tf.Variable(tf.truncated_normal([conv_ksize[0], conv_ksize[1], x_tensor_shape[3], conv_num_outputs], stddev=np.sqrt(2/x_tensor.shape[-1].value)))\n",
    "    bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    conv = tf.nn.conv2d(x_tensor, weights, strides=[1,conv_strides[0], conv_strides[1],1], padding='SAME')\n",
    "    conv = tf.nn.bias_add(conv, bias)\n",
    "    \n",
    "    conv = tf.nn.max_pool(conv, ksize=[1,pool_ksize[0], pool_ksize[1], 1], strides=[1,pool_strides[0], pool_strides[1],1], padding='SAME')\n",
    "    conv = tf.nn.relu(conv)\n",
    "    return conv\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    x_tensor_shape = x_tensor.get_shape().as_list()\n",
    "    x_tensor_newshape = [-1, np.prod(x_tensor_shape[1:])]\n",
    "    return tf.reshape(x_tensor, x_tensor_newshape)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    x_dims = x_tensor.get_shape().as_list()\n",
    "    weights = tf.Variable(tf.truncated_normal((x_dims[1], num_outputs), stddev=np.sqrt(2/x_tensor.shape[-1].value)))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    fully_con = tf.add(tf.matmul(x_tensor, weights),bias)\n",
    "    return tf.nn.relu(fully_con)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    x_dims = x_tensor.get_shape().as_list()\n",
    "    weights = tf.Variable(tf.truncated_normal((x_dims[1], num_outputs), stddev=np.sqrt(2/x_tensor.shape[-1].value)))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    return tf.add(tf.matmul(x_tensor, weights),bias)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv = conv2d_maxpool(x, 16, (3,3), (1,1), (1,1), (2,2))\n",
    "    conv = conv2d_maxpool(conv, 32,(3,3), (1,1), (1,1), (2,2))\n",
    "    conv = conv2d_maxpool(conv, 64,(3,3), (1,1), (2,2), (2,2))\n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    conv = flatten(conv)\n",
    "    \n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    conv = fully_conn(conv, 256)\n",
    "    conv = tf.nn.dropout(conv, keep_prob)\n",
    "    conv = fully_conn(conv, 32)\n",
    "    conv = tf.nn.dropout(conv, keep_prob)\n",
    "    \n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    out = output(conv,10)\n",
    "    \n",
    "    \n",
    "    # TODO: return output\n",
    "    return out\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer, feed_dict={x:feature_batch, y:label_batch, keep_prob:keep_probability})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    output = session.run(cost, feed_dict={x:feature_batch, y:label_batch, keep_prob:1.0})\n",
    "    print(\"The loss was: {0:.4f}\".format(output))\n",
    "    output = session.run(accuracy, feed_dict={x:valid_features, y:valid_labels, keep_prob:1.0})\n",
    "    print(\"The validation accuracy was: {0:.4f}\".format(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Tune Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "batch_size = 256\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  The loss was: 2.3018\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch  2, CIFAR-10 Batch 1:  The loss was: 2.3019\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch  3, CIFAR-10 Batch 1:  The loss was: 2.3020\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch  4, CIFAR-10 Batch 1:  The loss was: 2.3021\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch  5, CIFAR-10 Batch 1:  The loss was: 2.3022\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch  6, CIFAR-10 Batch 1:  The loss was: 2.3022\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch  7, CIFAR-10 Batch 1:  The loss was: 2.3022\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch  8, CIFAR-10 Batch 1:  The loss was: 2.3022\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch  9, CIFAR-10 Batch 1:  The loss was: 2.3022\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 10, CIFAR-10 Batch 1:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 11, CIFAR-10 Batch 1:  The loss was: 2.3024\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 12, CIFAR-10 Batch 1:  The loss was: 2.3024\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 13, CIFAR-10 Batch 1:  The loss was: 2.3024\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 14, CIFAR-10 Batch 1:  The loss was: 2.3024\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 15, CIFAR-10 Batch 1:  The loss was: 2.3024\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 16, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 17, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 18, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 19, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 20, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 21, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 22, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 23, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 24, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 25, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 26, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 27, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 28, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 29, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 30, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 31, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 32, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 33, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 34, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 35, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 36, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 37, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 38, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 39, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 40, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 41, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 42, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 43, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 44, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 45, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 46, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 47, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 48, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 49, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 50, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 51, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 52, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 53, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 54, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 55, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 56, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 57, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 58, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 59, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 60, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 61, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 62, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 63, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 64, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 65, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 66, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 67, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 68, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 69, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 70, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 71, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 72, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 73, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 74, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 75, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 76, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 77, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 78, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 79, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 80, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 81, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 82, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 83, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 84, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 85, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 86, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 87, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 88, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 89, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 90, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 91, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 92, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 93, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 94, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 95, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 97, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 98, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 99, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 100, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 101, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 102, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 103, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 104, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 105, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 106, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 107, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 108, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 109, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 110, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 111, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 112, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 113, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 114, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 115, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 116, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 117, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 118, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 119, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 120, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 121, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 122, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 123, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 124, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 125, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 126, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 127, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 128, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 129, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 130, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 131, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 132, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 133, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 134, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 135, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 136, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 137, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 138, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 139, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 140, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 141, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 142, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 143, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 144, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 145, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 146, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 147, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 148, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 149, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 150, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 151, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 152, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 153, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 154, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 155, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 156, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 157, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 158, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 159, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 160, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 161, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 162, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 163, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 164, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 165, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 166, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 167, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 168, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 169, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 170, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 171, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 172, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 173, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 174, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 175, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 176, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 177, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 178, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 179, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 180, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 181, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 182, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 183, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 184, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 185, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 186, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 187, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 188, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 189, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 190, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 191, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 192, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 193, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 194, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 195, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 196, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 197, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 198, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 199, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 200, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 201, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 202, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 203, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 204, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 205, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 206, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 207, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 208, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 209, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 210, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 211, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 212, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 213, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 214, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 215, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 216, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 217, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 218, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 219, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 220, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 221, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 222, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 223, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 224, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 225, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 226, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 227, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 228, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 229, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 230, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 231, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 232, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 233, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 234, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 235, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 236, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 237, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 238, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 239, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 240, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 241, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 242, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 243, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 244, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 245, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 246, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 247, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 248, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 249, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 250, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 251, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 252, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 253, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 254, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 255, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 256, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 257, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 258, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 259, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 260, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 261, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 262, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 263, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 264, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 265, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 266, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 267, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 268, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 269, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 270, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 271, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 272, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 273, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 274, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 275, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 276, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 277, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 278, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 279, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 280, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 281, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 282, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 283, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 284, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 285, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 286, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 287, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 288, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 289, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 290, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 291, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 292, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 293, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 294, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 295, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 296, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 297, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 298, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 299, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 300, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 301, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 302, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 303, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 304, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 305, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 306, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 307, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 308, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 309, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 310, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 311, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 312, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 313, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 314, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 315, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 316, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 317, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 318, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 319, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 320, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 321, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 322, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 323, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 324, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 325, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 326, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 327, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 328, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 329, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 330, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 331, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 332, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 333, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 334, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 335, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 336, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 337, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 338, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 339, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 340, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 341, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 342, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 343, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 344, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 345, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 346, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 347, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 348, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 349, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 350, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 351, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 352, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 353, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 354, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 355, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 356, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 357, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 358, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 359, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 360, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 361, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 362, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 363, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 364, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 365, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 366, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 367, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 368, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 369, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 370, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 371, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 372, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 373, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 374, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 375, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 376, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 377, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 378, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 379, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 380, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 381, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 382, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 383, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 384, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 385, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 386, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 387, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 388, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 389, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 390, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 391, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 392, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 393, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 394, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 395, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 396, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 397, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 398, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 399, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 400, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 401, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 402, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 403, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 404, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 405, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 406, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 407, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 408, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 409, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 410, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 411, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 412, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 413, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 414, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 415, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 416, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 417, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 418, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 419, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 420, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 421, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 422, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 423, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 424, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 425, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 426, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0998\n",
      "Epoch 427, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 428, CIFAR-10 Batch 1:  The loss was: 2.3026\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 429, CIFAR-10 Batch 1:  The loss was: 2.3026\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 430, CIFAR-10 Batch 1:  The loss was: 2.3026\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 431, CIFAR-10 Batch 1:  The loss was: 2.3026\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 432, CIFAR-10 Batch 1:  The loss was: 2.3026\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 433, CIFAR-10 Batch 1:  The loss was: 2.3026\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 434, CIFAR-10 Batch 1:  The loss was: 2.3026\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 435, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 436, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 437, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 438, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 439, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 440, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 441, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 442, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 443, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 444, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 445, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 446, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 447, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 448, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 449, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 450, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 451, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 452, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 453, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 454, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 455, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 456, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 457, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 458, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 459, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 460, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 461, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 462, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 463, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 464, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 465, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 466, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 467, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 468, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 469, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 470, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 471, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 472, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 473, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 474, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 475, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 476, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 477, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 478, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 479, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 480, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 481, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 482, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 483, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 484, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 485, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 486, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 487, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 488, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 489, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 490, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 491, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 492, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 493, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 494, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 495, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 496, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 497, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 498, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 499, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 500, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 501, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 502, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 503, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 504, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 505, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 506, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 507, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 508, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 509, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 510, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 511, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 512, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 513, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 514, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 515, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 516, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 517, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 518, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 519, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 520, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 521, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 522, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 523, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 524, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 525, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 526, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 527, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 528, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 529, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 530, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 531, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 532, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 533, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 534, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 535, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 536, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 537, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 538, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 539, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 540, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 541, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 542, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 543, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 544, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 545, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 546, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 547, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 548, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 549, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 550, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 551, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 552, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 553, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 554, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 555, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 556, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 557, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 558, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 559, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 560, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 561, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 562, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 563, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 564, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 565, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 566, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 567, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 568, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 569, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 570, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 571, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 572, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 573, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 574, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 575, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 576, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 577, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 578, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 579, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 580, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 581, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 582, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 583, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 584, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 585, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 586, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 587, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 588, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 589, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 590, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 591, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 592, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 593, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 594, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 595, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 596, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 597, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 598, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 599, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 600, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 601, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 602, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 603, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 604, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 605, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 606, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 607, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 608, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 609, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 610, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 611, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 612, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 613, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 614, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 615, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 616, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 617, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 618, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 619, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 620, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 621, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 622, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 623, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 624, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 625, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 626, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 627, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 628, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 629, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 630, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 631, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 632, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 633, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 634, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 635, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 636, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 637, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 638, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 639, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 640, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 641, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 642, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 643, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 644, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 645, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 646, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 647, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 648, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 649, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 650, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 651, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 652, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 653, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 654, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 655, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 656, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 657, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 658, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 659, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 660, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 661, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 662, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 663, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 664, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 665, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 666, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 667, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 668, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 669, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 670, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 671, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 672, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 673, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 674, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 675, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 676, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 677, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 678, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 679, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 680, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 681, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 682, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 683, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 684, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 685, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 686, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 687, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 688, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 689, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 690, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 691, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 692, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 693, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 694, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 695, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 696, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 697, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 698, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 699, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 700, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 701, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 702, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 703, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 704, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 705, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 706, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 707, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 708, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 709, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 710, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 711, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 712, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 713, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 714, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 715, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 716, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 717, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 718, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 719, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 720, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 721, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 722, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 723, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 724, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 725, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 726, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 727, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 728, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 729, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 730, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 731, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 732, CIFAR-10 Batch 1:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch 733, CIFAR-10 Batch 1:  The loss was: 2.2847\n",
      "The validation accuracy was: 0.1006\n",
      "Epoch 734, CIFAR-10 Batch 1:  The loss was: 2.2628\n",
      "The validation accuracy was: 0.1648\n",
      "Epoch 735, CIFAR-10 Batch 1:  The loss was: 2.1550\n",
      "The validation accuracy was: 0.1802\n",
      "Epoch 736, CIFAR-10 Batch 1:  The loss was: 2.0836\n",
      "The validation accuracy was: 0.2298\n",
      "Epoch 737, CIFAR-10 Batch 1:  The loss was: 1.9844\n",
      "The validation accuracy was: 0.2478\n",
      "Epoch 738, CIFAR-10 Batch 1:  The loss was: 1.8829\n",
      "The validation accuracy was: 0.2802\n",
      "Epoch 739, CIFAR-10 Batch 1:  The loss was: 1.7288\n",
      "The validation accuracy was: 0.2804\n",
      "Epoch 740, CIFAR-10 Batch 1:  The loss was: 1.6868\n",
      "The validation accuracy was: 0.3034\n",
      "Epoch 741, CIFAR-10 Batch 1:  The loss was: 1.6405\n",
      "The validation accuracy was: 0.3128\n",
      "Epoch 742, CIFAR-10 Batch 1:  The loss was: 1.5618\n",
      "The validation accuracy was: 0.3190\n",
      "Epoch 743, CIFAR-10 Batch 1:  The loss was: 1.5088\n",
      "The validation accuracy was: 0.3122\n",
      "Epoch 744, CIFAR-10 Batch 1:  The loss was: 1.3541\n",
      "The validation accuracy was: 0.3218\n",
      "Epoch 745, CIFAR-10 Batch 1:  The loss was: 1.2998\n",
      "The validation accuracy was: 0.3300\n",
      "Epoch 746, CIFAR-10 Batch 1:  The loss was: 1.2263\n",
      "The validation accuracy was: 0.3532\n",
      "Epoch 747, CIFAR-10 Batch 1:  The loss was: 1.1348\n",
      "The validation accuracy was: 0.3576\n",
      "Epoch 748, CIFAR-10 Batch 1:  The loss was: 1.0997\n",
      "The validation accuracy was: 0.3572\n",
      "Epoch 749, CIFAR-10 Batch 1:  The loss was: 1.0489\n",
      "The validation accuracy was: 0.3716\n",
      "Epoch 750, CIFAR-10 Batch 1:  The loss was: 0.9655\n",
      "The validation accuracy was: 0.3794\n",
      "Epoch 751, CIFAR-10 Batch 1:  The loss was: 0.8739\n",
      "The validation accuracy was: 0.3862\n",
      "Epoch 752, CIFAR-10 Batch 1:  The loss was: 0.8214\n",
      "The validation accuracy was: 0.3852\n",
      "Epoch 753, CIFAR-10 Batch 1:  The loss was: 0.7745\n",
      "The validation accuracy was: 0.3932\n",
      "Epoch 754, CIFAR-10 Batch 1:  The loss was: 0.6878\n",
      "The validation accuracy was: 0.3842\n",
      "Epoch 755, CIFAR-10 Batch 1:  The loss was: 0.6519\n",
      "The validation accuracy was: 0.3900\n",
      "Epoch 756, CIFAR-10 Batch 1:  The loss was: 0.5912\n",
      "The validation accuracy was: 0.3972\n",
      "Epoch 757, CIFAR-10 Batch 1:  The loss was: 0.5827\n",
      "The validation accuracy was: 0.3992\n",
      "Epoch 758, CIFAR-10 Batch 1:  The loss was: 0.5381\n",
      "The validation accuracy was: 0.4066\n",
      "Epoch 759, CIFAR-10 Batch 1:  The loss was: 0.5256\n",
      "The validation accuracy was: 0.4010\n",
      "Epoch 760, CIFAR-10 Batch 1:  The loss was: 0.5049\n",
      "The validation accuracy was: 0.4012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 761, CIFAR-10 Batch 1:  The loss was: 0.4709\n",
      "The validation accuracy was: 0.4048\n",
      "Epoch 762, CIFAR-10 Batch 1:  The loss was: 0.4636\n",
      "The validation accuracy was: 0.4028\n",
      "Epoch 763, CIFAR-10 Batch 1:  The loss was: 0.4470\n",
      "The validation accuracy was: 0.4040\n",
      "Epoch 764, CIFAR-10 Batch 1:  The loss was: 0.4359\n",
      "The validation accuracy was: 0.3868\n",
      "Epoch 765, CIFAR-10 Batch 1:  The loss was: 0.4288\n",
      "The validation accuracy was: 0.3914\n",
      "Epoch 766, CIFAR-10 Batch 1:  The loss was: 0.3856\n",
      "The validation accuracy was: 0.4166\n",
      "Epoch 767, CIFAR-10 Batch 1:  The loss was: 0.3734\n",
      "The validation accuracy was: 0.4094\n",
      "Epoch 768, CIFAR-10 Batch 1:  The loss was: 0.3418\n",
      "The validation accuracy was: 0.4150\n",
      "Epoch 769, CIFAR-10 Batch 1:  The loss was: 0.3347\n",
      "The validation accuracy was: 0.4162\n",
      "Epoch 770, CIFAR-10 Batch 1:  The loss was: 0.3047\n",
      "The validation accuracy was: 0.4180\n",
      "Epoch 771, CIFAR-10 Batch 1:  The loss was: 0.2896\n",
      "The validation accuracy was: 0.4190\n",
      "Epoch 772, CIFAR-10 Batch 1:  The loss was: 0.2973\n",
      "The validation accuracy was: 0.4238\n",
      "Epoch 773, CIFAR-10 Batch 1:  The loss was: 0.2795\n",
      "The validation accuracy was: 0.4208\n",
      "Epoch 774, CIFAR-10 Batch 1:  The loss was: 0.2535\n",
      "The validation accuracy was: 0.4290\n",
      "Epoch 775, CIFAR-10 Batch 1:  The loss was: 0.2746\n",
      "The validation accuracy was: 0.4258\n",
      "Epoch 776, CIFAR-10 Batch 1:  The loss was: 0.2561\n",
      "The validation accuracy was: 0.4284\n",
      "Epoch 777, CIFAR-10 Batch 1:  The loss was: 0.2642\n",
      "The validation accuracy was: 0.4268\n",
      "Epoch 778, CIFAR-10 Batch 1:  The loss was: 0.2547\n",
      "The validation accuracy was: 0.4186\n",
      "Epoch 779, CIFAR-10 Batch 1:  The loss was: 0.2258\n",
      "The validation accuracy was: 0.4256\n",
      "Epoch 780, CIFAR-10 Batch 1:  The loss was: 0.2123\n",
      "The validation accuracy was: 0.4314\n",
      "Epoch 781, CIFAR-10 Batch 1:  The loss was: 0.2079\n",
      "The validation accuracy was: 0.4272\n",
      "Epoch 782, CIFAR-10 Batch 1:  The loss was: 0.2125\n",
      "The validation accuracy was: 0.4264\n",
      "Epoch 783, CIFAR-10 Batch 1:  The loss was: 0.2099\n",
      "The validation accuracy was: 0.4258\n",
      "Epoch 784, CIFAR-10 Batch 1:  The loss was: 0.2000\n",
      "The validation accuracy was: 0.4242\n",
      "Epoch 785, CIFAR-10 Batch 1:  The loss was: 0.1981\n",
      "The validation accuracy was: 0.4306\n",
      "Epoch 786, CIFAR-10 Batch 1:  The loss was: 0.1932\n",
      "The validation accuracy was: 0.4336\n",
      "Epoch 787, CIFAR-10 Batch 1:  The loss was: 0.1861\n",
      "The validation accuracy was: 0.4320\n",
      "Epoch 788, CIFAR-10 Batch 1:  The loss was: 0.1822\n",
      "The validation accuracy was: 0.4282\n",
      "Epoch 789, CIFAR-10 Batch 1:  The loss was: 0.1764\n",
      "The validation accuracy was: 0.4324\n",
      "Epoch 790, CIFAR-10 Batch 1:  The loss was: 0.1684\n",
      "The validation accuracy was: 0.4370\n",
      "Epoch 791, CIFAR-10 Batch 1:  The loss was: 0.1549\n",
      "The validation accuracy was: 0.4256\n",
      "Epoch 792, CIFAR-10 Batch 1:  The loss was: 0.1653\n",
      "The validation accuracy was: 0.4294\n",
      "Epoch 793, CIFAR-10 Batch 1:  The loss was: 0.1413\n",
      "The validation accuracy was: 0.4340\n",
      "Epoch 794, CIFAR-10 Batch 1:  The loss was: 0.1267\n",
      "The validation accuracy was: 0.4326\n",
      "Epoch 795, CIFAR-10 Batch 1:  The loss was: 0.1208\n",
      "The validation accuracy was: 0.4348\n",
      "Epoch 796, CIFAR-10 Batch 1:  The loss was: 0.1187\n",
      "The validation accuracy was: 0.4326\n",
      "Epoch 797, CIFAR-10 Batch 1:  The loss was: 0.1137\n",
      "The validation accuracy was: 0.4320\n",
      "Epoch 798, CIFAR-10 Batch 1:  The loss was: 0.1090\n",
      "The validation accuracy was: 0.4328\n",
      "Epoch 799, CIFAR-10 Batch 1:  The loss was: 0.1007\n",
      "The validation accuracy was: 0.4306\n",
      "Epoch 800, CIFAR-10 Batch 1:  The loss was: 0.0959\n",
      "The validation accuracy was: 0.4332\n",
      "Epoch 801, CIFAR-10 Batch 1:  The loss was: 0.0958\n",
      "The validation accuracy was: 0.4296\n",
      "Epoch 802, CIFAR-10 Batch 1:  The loss was: 0.0945\n",
      "The validation accuracy was: 0.4282\n",
      "Epoch 803, CIFAR-10 Batch 1:  The loss was: 0.1006\n",
      "The validation accuracy was: 0.4294\n",
      "Epoch 804, CIFAR-10 Batch 1:  The loss was: 0.0937\n",
      "The validation accuracy was: 0.4246\n",
      "Epoch 805, CIFAR-10 Batch 1:  The loss was: 0.0949\n",
      "The validation accuracy was: 0.4298\n",
      "Epoch 806, CIFAR-10 Batch 1:  The loss was: 0.0730\n",
      "The validation accuracy was: 0.4236\n",
      "Epoch 807, CIFAR-10 Batch 1:  The loss was: 0.0770\n",
      "The validation accuracy was: 0.4320\n",
      "Epoch 808, CIFAR-10 Batch 1:  The loss was: 0.0761\n",
      "The validation accuracy was: 0.4320\n",
      "Epoch 809, CIFAR-10 Batch 1:  The loss was: 0.0679\n",
      "The validation accuracy was: 0.4226\n",
      "Epoch 810, CIFAR-10 Batch 1:  The loss was: 0.0695\n",
      "The validation accuracy was: 0.4288\n",
      "Epoch 811, CIFAR-10 Batch 1:  The loss was: 0.0636\n",
      "The validation accuracy was: 0.4234\n",
      "Epoch 812, CIFAR-10 Batch 1:  The loss was: 0.0557\n",
      "The validation accuracy was: 0.4246\n",
      "Epoch 813, CIFAR-10 Batch 1:  The loss was: 0.0572\n",
      "The validation accuracy was: 0.4278\n",
      "Epoch 814, CIFAR-10 Batch 1:  The loss was: 0.0571\n",
      "The validation accuracy was: 0.4252\n",
      "Epoch 815, CIFAR-10 Batch 1:  The loss was: 0.0557\n",
      "The validation accuracy was: 0.4236\n",
      "Epoch 816, CIFAR-10 Batch 1:  The loss was: 0.0518\n",
      "The validation accuracy was: 0.4262\n",
      "Epoch 817, CIFAR-10 Batch 1:  The loss was: 0.0497\n",
      "The validation accuracy was: 0.4342\n",
      "Epoch 818, CIFAR-10 Batch 1:  The loss was: 0.0450\n",
      "The validation accuracy was: 0.4354\n",
      "Epoch 819, CIFAR-10 Batch 1:  The loss was: 0.0498\n",
      "The validation accuracy was: 0.4336\n",
      "Epoch 820, CIFAR-10 Batch 1:  The loss was: 0.0444\n",
      "The validation accuracy was: 0.4368\n",
      "Epoch 821, CIFAR-10 Batch 1:  The loss was: 0.0396\n",
      "The validation accuracy was: 0.4314\n",
      "Epoch 822, CIFAR-10 Batch 1:  The loss was: 0.0385\n",
      "The validation accuracy was: 0.4362\n",
      "Epoch 823, CIFAR-10 Batch 1:  The loss was: 0.0363\n",
      "The validation accuracy was: 0.4384\n",
      "Epoch 824, CIFAR-10 Batch 1:  The loss was: 0.0347\n",
      "The validation accuracy was: 0.4340\n",
      "Epoch 825, CIFAR-10 Batch 1:  The loss was: 0.0303\n",
      "The validation accuracy was: 0.4306\n",
      "Epoch 826, CIFAR-10 Batch 1:  The loss was: 0.0314\n",
      "The validation accuracy was: 0.4318\n",
      "Epoch 827, CIFAR-10 Batch 1:  The loss was: 0.0316\n",
      "The validation accuracy was: 0.4246\n",
      "Epoch 828, CIFAR-10 Batch 1:  The loss was: 0.0319\n",
      "The validation accuracy was: 0.4352\n",
      "Epoch 829, CIFAR-10 Batch 1:  The loss was: 0.0327\n",
      "The validation accuracy was: 0.4352\n",
      "Epoch 830, CIFAR-10 Batch 1:  The loss was: 0.0259\n",
      "The validation accuracy was: 0.4340\n",
      "Epoch 831, CIFAR-10 Batch 1:  The loss was: 0.0219\n",
      "The validation accuracy was: 0.4278\n",
      "Epoch 832, CIFAR-10 Batch 1:  The loss was: 0.0246\n",
      "The validation accuracy was: 0.4312\n",
      "Epoch 833, CIFAR-10 Batch 1:  The loss was: 0.0251\n",
      "The validation accuracy was: 0.4284\n",
      "Epoch 834, CIFAR-10 Batch 1:  The loss was: 0.0215\n",
      "The validation accuracy was: 0.4298\n",
      "Epoch 835, CIFAR-10 Batch 1:  The loss was: 0.0209\n",
      "The validation accuracy was: 0.4288\n",
      "Epoch 836, CIFAR-10 Batch 1:  The loss was: 0.0192\n",
      "The validation accuracy was: 0.4276\n",
      "Epoch 837, CIFAR-10 Batch 1:  The loss was: 0.0173\n",
      "The validation accuracy was: 0.4280\n",
      "Epoch 838, CIFAR-10 Batch 1:  The loss was: 0.0176\n",
      "The validation accuracy was: 0.4242\n",
      "Epoch 839, CIFAR-10 Batch 1:  The loss was: 0.0156\n",
      "The validation accuracy was: 0.4304\n",
      "Epoch 840, CIFAR-10 Batch 1:  The loss was: 0.0137\n",
      "The validation accuracy was: 0.4264\n",
      "Epoch 841, CIFAR-10 Batch 1:  The loss was: 0.0123\n",
      "The validation accuracy was: 0.4296\n",
      "Epoch 842, CIFAR-10 Batch 1:  The loss was: 0.0155\n",
      "The validation accuracy was: 0.4238\n",
      "Epoch 843, CIFAR-10 Batch 1:  The loss was: 0.0127\n",
      "The validation accuracy was: 0.4276\n",
      "Epoch 844, CIFAR-10 Batch 1:  The loss was: 0.0132\n",
      "The validation accuracy was: 0.4228\n",
      "Epoch 845, CIFAR-10 Batch 1:  The loss was: 0.0128\n",
      "The validation accuracy was: 0.4298\n",
      "Epoch 846, CIFAR-10 Batch 1:  The loss was: 0.0117\n",
      "The validation accuracy was: 0.4258\n",
      "Epoch 847, CIFAR-10 Batch 1:  The loss was: 0.0118\n",
      "The validation accuracy was: 0.4202\n",
      "Epoch 848, CIFAR-10 Batch 1:  The loss was: 0.0115\n",
      "The validation accuracy was: 0.4246\n",
      "Epoch 849, CIFAR-10 Batch 1:  The loss was: 0.0108\n",
      "The validation accuracy was: 0.4258\n",
      "Epoch 850, CIFAR-10 Batch 1:  The loss was: 0.0091\n",
      "The validation accuracy was: 0.4310\n",
      "Epoch 851, CIFAR-10 Batch 1:  The loss was: 0.0124\n",
      "The validation accuracy was: 0.4308\n",
      "Epoch 852, CIFAR-10 Batch 1:  The loss was: 0.0098\n",
      "The validation accuracy was: 0.4232\n",
      "Epoch 853, CIFAR-10 Batch 1:  The loss was: 0.0100\n",
      "The validation accuracy was: 0.4264\n",
      "Epoch 854, CIFAR-10 Batch 1:  The loss was: 0.0073\n",
      "The validation accuracy was: 0.4258\n",
      "Epoch 855, CIFAR-10 Batch 1:  The loss was: 0.0092\n",
      "The validation accuracy was: 0.4312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 856, CIFAR-10 Batch 1:  The loss was: 0.0075\n",
      "The validation accuracy was: 0.4214\n",
      "Epoch 857, CIFAR-10 Batch 1:  The loss was: 0.0062\n",
      "The validation accuracy was: 0.4296\n",
      "Epoch 858, CIFAR-10 Batch 1:  The loss was: 0.0064\n",
      "The validation accuracy was: 0.4292\n",
      "Epoch 859, CIFAR-10 Batch 1:  The loss was: 0.0060\n",
      "The validation accuracy was: 0.4306\n",
      "Epoch 860, CIFAR-10 Batch 1:  The loss was: 0.0063\n",
      "The validation accuracy was: 0.4286\n",
      "Epoch 861, CIFAR-10 Batch 1:  The loss was: 0.0054\n",
      "The validation accuracy was: 0.4348\n",
      "Epoch 862, CIFAR-10 Batch 1:  The loss was: 0.0071\n",
      "The validation accuracy was: 0.4316\n",
      "Epoch 863, CIFAR-10 Batch 1:  The loss was: 0.0066\n",
      "The validation accuracy was: 0.4316\n",
      "Epoch 864, CIFAR-10 Batch 1:  The loss was: 0.0058\n",
      "The validation accuracy was: 0.4288\n",
      "Epoch 865, CIFAR-10 Batch 1:  The loss was: 0.0055\n",
      "The validation accuracy was: 0.4254\n",
      "Epoch 866, CIFAR-10 Batch 1:  The loss was: 0.0050\n",
      "The validation accuracy was: 0.4288\n",
      "Epoch 867, CIFAR-10 Batch 1:  The loss was: 0.0049\n",
      "The validation accuracy was: 0.4276\n",
      "Epoch 868, CIFAR-10 Batch 1:  The loss was: 0.0045\n",
      "The validation accuracy was: 0.4284\n",
      "Epoch 869, CIFAR-10 Batch 1:  The loss was: 0.0060\n",
      "The validation accuracy was: 0.4348\n",
      "Epoch 870, CIFAR-10 Batch 1:  The loss was: 0.0043\n",
      "The validation accuracy was: 0.4336\n",
      "Epoch 871, CIFAR-10 Batch 1:  The loss was: 0.0043\n",
      "The validation accuracy was: 0.4318\n",
      "Epoch 872, CIFAR-10 Batch 1:  The loss was: 0.0032\n",
      "The validation accuracy was: 0.4344\n",
      "Epoch 873, CIFAR-10 Batch 1:  The loss was: 0.0056\n",
      "The validation accuracy was: 0.4310\n",
      "Epoch 874, CIFAR-10 Batch 1:  The loss was: 0.0052\n",
      "The validation accuracy was: 0.4366\n",
      "Epoch 875, CIFAR-10 Batch 1:  The loss was: 0.0033\n",
      "The validation accuracy was: 0.4290\n",
      "Epoch 876, CIFAR-10 Batch 1:  The loss was: 0.0041\n",
      "The validation accuracy was: 0.4352\n",
      "Epoch 877, CIFAR-10 Batch 1:  The loss was: 0.0029\n",
      "The validation accuracy was: 0.4298\n",
      "Epoch 878, CIFAR-10 Batch 1:  The loss was: 0.0024\n",
      "The validation accuracy was: 0.4268\n",
      "Epoch 879, CIFAR-10 Batch 1:  The loss was: 0.0028\n",
      "The validation accuracy was: 0.4356\n",
      "Epoch 880, CIFAR-10 Batch 1:  The loss was: 0.0023\n",
      "The validation accuracy was: 0.4288\n",
      "Epoch 881, CIFAR-10 Batch 1:  The loss was: 0.0036\n",
      "The validation accuracy was: 0.4272\n",
      "Epoch 882, CIFAR-10 Batch 1:  The loss was: 0.0025\n",
      "The validation accuracy was: 0.4290\n",
      "Epoch 883, CIFAR-10 Batch 1:  The loss was: 0.0030\n",
      "The validation accuracy was: 0.4332\n",
      "Epoch 884, CIFAR-10 Batch 1:  The loss was: 0.0031\n",
      "The validation accuracy was: 0.4320\n",
      "Epoch 885, CIFAR-10 Batch 1:  The loss was: 0.0027\n",
      "The validation accuracy was: 0.4324\n",
      "Epoch 886, CIFAR-10 Batch 1:  The loss was: 0.0034\n",
      "The validation accuracy was: 0.4298\n",
      "Epoch 887, CIFAR-10 Batch 1:  The loss was: 0.0029\n",
      "The validation accuracy was: 0.4308\n",
      "Epoch 888, CIFAR-10 Batch 1:  The loss was: 0.0029\n",
      "The validation accuracy was: 0.4310\n",
      "Epoch 889, CIFAR-10 Batch 1:  The loss was: 0.0021\n",
      "The validation accuracy was: 0.4320\n",
      "Epoch 890, CIFAR-10 Batch 1:  The loss was: 0.0041\n",
      "The validation accuracy was: 0.4318\n",
      "Epoch 891, CIFAR-10 Batch 1:  The loss was: 0.0032\n",
      "The validation accuracy was: 0.4334\n",
      "Epoch 892, CIFAR-10 Batch 1:  The loss was: 0.0025\n",
      "The validation accuracy was: 0.4288\n",
      "Epoch 893, CIFAR-10 Batch 1:  The loss was: 0.0033\n",
      "The validation accuracy was: 0.4326\n",
      "Epoch 894, CIFAR-10 Batch 1:  The loss was: 0.0025\n",
      "The validation accuracy was: 0.4348\n",
      "Epoch 895, CIFAR-10 Batch 1:  The loss was: 0.0025\n",
      "The validation accuracy was: 0.4382\n",
      "Epoch 896, CIFAR-10 Batch 1:  The loss was: 0.0026\n",
      "The validation accuracy was: 0.4324\n",
      "Epoch 897, CIFAR-10 Batch 1:  The loss was: 0.0028\n",
      "The validation accuracy was: 0.4334\n",
      "Epoch 898, CIFAR-10 Batch 1:  The loss was: 0.0022\n",
      "The validation accuracy was: 0.4342\n",
      "Epoch 899, CIFAR-10 Batch 1:  The loss was: 0.0032\n",
      "The validation accuracy was: 0.4368\n",
      "Epoch 900, CIFAR-10 Batch 1:  The loss was: 0.0029\n",
      "The validation accuracy was: 0.4322\n",
      "Epoch 901, CIFAR-10 Batch 1:  The loss was: 0.0017\n",
      "The validation accuracy was: 0.4312\n",
      "Epoch 902, CIFAR-10 Batch 1:  The loss was: 0.0022\n",
      "The validation accuracy was: 0.4318\n",
      "Epoch 903, CIFAR-10 Batch 1:  The loss was: 0.0017\n",
      "The validation accuracy was: 0.4334\n",
      "Epoch 904, CIFAR-10 Batch 1:  The loss was: 0.0016\n",
      "The validation accuracy was: 0.4330\n",
      "Epoch 905, CIFAR-10 Batch 1:  The loss was: 0.0016\n",
      "The validation accuracy was: 0.4282\n",
      "Epoch 906, CIFAR-10 Batch 1:  The loss was: 0.0012\n",
      "The validation accuracy was: 0.4312\n",
      "Epoch 907, CIFAR-10 Batch 1:  The loss was: 0.0012\n",
      "The validation accuracy was: 0.4326\n",
      "Epoch 908, CIFAR-10 Batch 1:  The loss was: 0.0014\n",
      "The validation accuracy was: 0.4346\n",
      "Epoch 909, CIFAR-10 Batch 1:  The loss was: 0.0010\n",
      "The validation accuracy was: 0.4268\n",
      "Epoch 910, CIFAR-10 Batch 1:  The loss was: 0.0020\n",
      "The validation accuracy was: 0.4288\n",
      "Epoch 911, CIFAR-10 Batch 1:  The loss was: 0.0023\n",
      "The validation accuracy was: 0.4290\n",
      "Epoch 912, CIFAR-10 Batch 1:  The loss was: 0.0013\n",
      "The validation accuracy was: 0.4318\n",
      "Epoch 913, CIFAR-10 Batch 1:  The loss was: 0.0024\n",
      "The validation accuracy was: 0.4286\n",
      "Epoch 914, CIFAR-10 Batch 1:  The loss was: 0.0010\n",
      "The validation accuracy was: 0.4284\n",
      "Epoch 915, CIFAR-10 Batch 1:  The loss was: 0.0012\n",
      "The validation accuracy was: 0.4362\n",
      "Epoch 916, CIFAR-10 Batch 1:  The loss was: 0.0012\n",
      "The validation accuracy was: 0.4374\n",
      "Epoch 917, CIFAR-10 Batch 1:  The loss was: 0.0013\n",
      "The validation accuracy was: 0.4358\n",
      "Epoch 918, CIFAR-10 Batch 1:  The loss was: 0.0037\n",
      "The validation accuracy was: 0.4298\n",
      "Epoch 919, CIFAR-10 Batch 1:  The loss was: 0.0009\n",
      "The validation accuracy was: 0.4410\n",
      "Epoch 920, CIFAR-10 Batch 1:  The loss was: 0.0010\n",
      "The validation accuracy was: 0.4400\n",
      "Epoch 921, CIFAR-10 Batch 1:  The loss was: 0.0008\n",
      "The validation accuracy was: 0.4364\n",
      "Epoch 922, CIFAR-10 Batch 1:  The loss was: 0.0011\n",
      "The validation accuracy was: 0.4358\n",
      "Epoch 923, CIFAR-10 Batch 1:  The loss was: 0.0011\n",
      "The validation accuracy was: 0.4322\n",
      "Epoch 924, CIFAR-10 Batch 1:  The loss was: 0.0008\n",
      "The validation accuracy was: 0.4402\n",
      "Epoch 925, CIFAR-10 Batch 1:  The loss was: 0.0007\n",
      "The validation accuracy was: 0.4366\n",
      "Epoch 926, CIFAR-10 Batch 1:  The loss was: 0.0009\n",
      "The validation accuracy was: 0.4322\n",
      "Epoch 927, CIFAR-10 Batch 1:  The loss was: 0.0007\n",
      "The validation accuracy was: 0.4368\n",
      "Epoch 928, CIFAR-10 Batch 1:  The loss was: 0.0013\n",
      "The validation accuracy was: 0.4378\n",
      "Epoch 929, CIFAR-10 Batch 1:  The loss was: 0.0012\n",
      "The validation accuracy was: 0.4390\n",
      "Epoch 930, CIFAR-10 Batch 1:  The loss was: 0.0008\n",
      "The validation accuracy was: 0.4314\n",
      "Epoch 931, CIFAR-10 Batch 1:  The loss was: 0.0006\n",
      "The validation accuracy was: 0.4350\n",
      "Epoch 932, CIFAR-10 Batch 1:  The loss was: 0.0005\n",
      "The validation accuracy was: 0.4366\n",
      "Epoch 933, CIFAR-10 Batch 1:  The loss was: 0.0005\n",
      "The validation accuracy was: 0.4316\n",
      "Epoch 934, CIFAR-10 Batch 1:  The loss was: 0.0008\n",
      "The validation accuracy was: 0.4322\n",
      "Epoch 935, CIFAR-10 Batch 1:  The loss was: 0.0005\n",
      "The validation accuracy was: 0.4302\n",
      "Epoch 936, CIFAR-10 Batch 1:  The loss was: 0.0006\n",
      "The validation accuracy was: 0.4396\n",
      "Epoch 937, CIFAR-10 Batch 1:  The loss was: 0.0007\n",
      "The validation accuracy was: 0.4336\n",
      "Epoch 938, CIFAR-10 Batch 1:  The loss was: 0.0005\n",
      "The validation accuracy was: 0.4390\n",
      "Epoch 939, CIFAR-10 Batch 1:  The loss was: 0.0004\n",
      "The validation accuracy was: 0.4366\n",
      "Epoch 940, CIFAR-10 Batch 1:  The loss was: 0.0007\n",
      "The validation accuracy was: 0.4292\n",
      "Epoch 941, CIFAR-10 Batch 1:  The loss was: 0.0003\n",
      "The validation accuracy was: 0.4384\n",
      "Epoch 942, CIFAR-10 Batch 1:  The loss was: 0.0006\n",
      "The validation accuracy was: 0.4378\n",
      "Epoch 943, CIFAR-10 Batch 1:  The loss was: 0.0008\n",
      "The validation accuracy was: 0.4368\n",
      "Epoch 944, CIFAR-10 Batch 1:  The loss was: 0.0004\n",
      "The validation accuracy was: 0.4428\n",
      "Epoch 945, CIFAR-10 Batch 1:  The loss was: 0.0004\n",
      "The validation accuracy was: 0.4414\n",
      "Epoch 946, CIFAR-10 Batch 1:  The loss was: 0.0006\n",
      "The validation accuracy was: 0.4386\n",
      "Epoch 947, CIFAR-10 Batch 1:  The loss was: 0.0004\n",
      "The validation accuracy was: 0.4376\n",
      "Epoch 948, CIFAR-10 Batch 1:  The loss was: 0.0002\n",
      "The validation accuracy was: 0.4336\n",
      "Epoch 949, CIFAR-10 Batch 1:  The loss was: 0.0003\n",
      "The validation accuracy was: 0.4344\n",
      "Epoch 950, CIFAR-10 Batch 1:  The loss was: 0.0004\n",
      "The validation accuracy was: 0.4400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 951, CIFAR-10 Batch 1:  The loss was: 0.0002\n",
      "The validation accuracy was: 0.4410\n",
      "Epoch 952, CIFAR-10 Batch 1:  The loss was: 0.0003\n",
      "The validation accuracy was: 0.4332\n",
      "Epoch 953, CIFAR-10 Batch 1:  The loss was: 0.0003\n",
      "The validation accuracy was: 0.4356\n",
      "Epoch 954, CIFAR-10 Batch 1:  The loss was: 0.0007\n",
      "The validation accuracy was: 0.4366\n",
      "Epoch 955, CIFAR-10 Batch 1:  The loss was: 0.0004\n",
      "The validation accuracy was: 0.4384\n",
      "Epoch 956, CIFAR-10 Batch 1:  The loss was: 0.0001\n",
      "The validation accuracy was: 0.4376\n",
      "Epoch 957, CIFAR-10 Batch 1:  The loss was: 0.0005\n",
      "The validation accuracy was: 0.4358\n",
      "Epoch 958, CIFAR-10 Batch 1:  The loss was: 0.0003\n",
      "The validation accuracy was: 0.4380\n",
      "Epoch 959, CIFAR-10 Batch 1:  The loss was: 0.0002\n",
      "The validation accuracy was: 0.4378\n",
      "Epoch 960, CIFAR-10 Batch 1:  The loss was: 0.0002\n",
      "The validation accuracy was: 0.4340\n",
      "Epoch 961, CIFAR-10 Batch 1:  The loss was: 0.0003\n",
      "The validation accuracy was: 0.4336\n",
      "Epoch 962, CIFAR-10 Batch 1:  The loss was: 0.0004\n",
      "The validation accuracy was: 0.4340\n",
      "Epoch 963, CIFAR-10 Batch 1:  The loss was: 0.0005\n",
      "The validation accuracy was: 0.4322\n",
      "Epoch 964, CIFAR-10 Batch 1:  The loss was: 0.0002\n",
      "The validation accuracy was: 0.4320\n",
      "Epoch 965, CIFAR-10 Batch 1:  The loss was: 0.0005\n",
      "The validation accuracy was: 0.4296\n",
      "Epoch 966, CIFAR-10 Batch 1:  The loss was: 0.0003\n",
      "The validation accuracy was: 0.4278\n",
      "Epoch 967, CIFAR-10 Batch 1:  The loss was: 0.0003\n",
      "The validation accuracy was: 0.4324\n",
      "Epoch 968, CIFAR-10 Batch 1:  The loss was: 0.0004\n",
      "The validation accuracy was: 0.4342\n",
      "Epoch 969, CIFAR-10 Batch 1:  The loss was: 0.0003\n",
      "The validation accuracy was: 0.4354\n",
      "Epoch 970, CIFAR-10 Batch 1:  The loss was: 0.0003\n",
      "The validation accuracy was: 0.4366\n",
      "Epoch 971, CIFAR-10 Batch 1:  The loss was: 0.0003\n",
      "The validation accuracy was: 0.4318\n",
      "Epoch 972, CIFAR-10 Batch 1:  The loss was: 0.0002\n",
      "The validation accuracy was: 0.4352\n",
      "Epoch 973, CIFAR-10 Batch 1:  The loss was: 0.0001\n",
      "The validation accuracy was: 0.4370\n",
      "Epoch 974, CIFAR-10 Batch 1:  The loss was: 0.0015\n",
      "The validation accuracy was: 0.4344\n",
      "Epoch 975, CIFAR-10 Batch 1:  The loss was: 0.0001\n",
      "The validation accuracy was: 0.4310\n",
      "Epoch 976, CIFAR-10 Batch 1:  The loss was: 0.0002\n",
      "The validation accuracy was: 0.4350\n",
      "Epoch 977, CIFAR-10 Batch 1:  The loss was: 0.0003\n",
      "The validation accuracy was: 0.4330\n",
      "Epoch 978, CIFAR-10 Batch 1:  The loss was: 0.0002\n",
      "The validation accuracy was: 0.4332\n",
      "Epoch 979, CIFAR-10 Batch 1:  The loss was: 0.0001\n",
      "The validation accuracy was: 0.4386\n",
      "Epoch 980, CIFAR-10 Batch 1:  The loss was: 0.0001\n",
      "The validation accuracy was: 0.4310\n",
      "Epoch 981, CIFAR-10 Batch 1:  The loss was: 0.0001\n",
      "The validation accuracy was: 0.4410\n",
      "Epoch 982, CIFAR-10 Batch 1:  The loss was: 0.0001\n",
      "The validation accuracy was: 0.4360\n",
      "Epoch 983, CIFAR-10 Batch 1:  The loss was: 0.0001\n",
      "The validation accuracy was: 0.4336\n",
      "Epoch 984, CIFAR-10 Batch 1:  The loss was: 0.0001\n",
      "The validation accuracy was: 0.4366\n",
      "Epoch 985, CIFAR-10 Batch 1:  The loss was: 0.0001\n",
      "The validation accuracy was: 0.4352\n",
      "Epoch 986, CIFAR-10 Batch 1:  The loss was: 0.0001\n",
      "The validation accuracy was: 0.4340\n",
      "Epoch 987, CIFAR-10 Batch 1:  The loss was: 0.0001\n",
      "The validation accuracy was: 0.4362\n",
      "Epoch 988, CIFAR-10 Batch 1:  The loss was: 0.0001\n",
      "The validation accuracy was: 0.4334\n",
      "Epoch 989, CIFAR-10 Batch 1:  The loss was: 0.0001\n",
      "The validation accuracy was: 0.4318\n",
      "Epoch 990, CIFAR-10 Batch 1:  The loss was: 0.0000\n",
      "The validation accuracy was: 0.4406\n",
      "Epoch 991, CIFAR-10 Batch 1:  The loss was: 0.0000\n",
      "The validation accuracy was: 0.4350\n",
      "Epoch 992, CIFAR-10 Batch 1:  The loss was: 0.0000\n",
      "The validation accuracy was: 0.4356\n",
      "Epoch 993, CIFAR-10 Batch 1:  The loss was: 0.0001\n",
      "The validation accuracy was: 0.4316\n",
      "Epoch 994, CIFAR-10 Batch 1:  The loss was: 0.0001\n",
      "The validation accuracy was: 0.4384\n",
      "Epoch 995, CIFAR-10 Batch 1:  The loss was: 0.0000\n",
      "The validation accuracy was: 0.4328\n",
      "Epoch 996, CIFAR-10 Batch 1:  The loss was: 0.0001\n",
      "The validation accuracy was: 0.4312\n",
      "Epoch 997, CIFAR-10 Batch 1:  The loss was: 0.0001\n",
      "The validation accuracy was: 0.4322\n",
      "Epoch 998, CIFAR-10 Batch 1:  The loss was: 0.0000\n",
      "The validation accuracy was: 0.4382\n",
      "Epoch 999, CIFAR-10 Batch 1:  The loss was: 0.0000\n",
      "The validation accuracy was: 0.4336\n",
      "Epoch 1000, CIFAR-10 Batch 1:  The loss was: 0.0001\n",
      "The validation accuracy was: 0.4354\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  The loss was: 2.3036\n",
      "The validation accuracy was: 0.1050\n",
      "Epoch  1, CIFAR-10 Batch 2:  The loss was: 2.3030\n",
      "The validation accuracy was: 0.0970\n",
      "Epoch  1, CIFAR-10 Batch 3:  The loss was: 2.3029\n",
      "The validation accuracy was: 0.0998\n",
      "Epoch  1, CIFAR-10 Batch 4:  The loss was: 2.3028\n",
      "The validation accuracy was: 0.0970\n",
      "Epoch  1, CIFAR-10 Batch 5:  The loss was: 2.3029\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch  2, CIFAR-10 Batch 1:  The loss was: 2.3031\n",
      "The validation accuracy was: 0.0970\n",
      "Epoch  2, CIFAR-10 Batch 2:  The loss was: 2.3019\n",
      "The validation accuracy was: 0.0970\n",
      "Epoch  2, CIFAR-10 Batch 3:  The loss was: 2.3011\n",
      "The validation accuracy was: 0.0998\n",
      "Epoch  2, CIFAR-10 Batch 4:  The loss was: 2.3016\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch  2, CIFAR-10 Batch 5:  The loss was: 2.3026\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch  3, CIFAR-10 Batch 1:  The loss was: 2.3029\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch  3, CIFAR-10 Batch 2:  The loss was: 2.3015\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch  3, CIFAR-10 Batch 3:  The loss was: 2.3004\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch  3, CIFAR-10 Batch 4:  The loss was: 2.3011\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch  3, CIFAR-10 Batch 5:  The loss was: 2.3025\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch  4, CIFAR-10 Batch 1:  The loss was: 2.3028\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch  4, CIFAR-10 Batch 2:  The loss was: 2.3013\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch  4, CIFAR-10 Batch 3:  The loss was: 2.3001\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch  4, CIFAR-10 Batch 4:  The loss was: 2.3009\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch  4, CIFAR-10 Batch 5:  The loss was: 2.3024\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch  5, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch  5, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch  5, CIFAR-10 Batch 3:  The loss was: 2.3001\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch  5, CIFAR-10 Batch 4:  The loss was: 2.3008\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch  5, CIFAR-10 Batch 5:  The loss was: 2.3024\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch  6, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch  6, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch  6, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch  6, CIFAR-10 Batch 4:  The loss was: 2.3008\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch  6, CIFAR-10 Batch 5:  The loss was: 2.3024\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch  7, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch  7, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch  7, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch  7, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch  7, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch  8, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch  8, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch  8, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch  8, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch  8, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch  9, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch  9, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch  9, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch  9, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch  9, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 10, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 10, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 10, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 10, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 10, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 11, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 11, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 11, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 11, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 11, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 12, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 12, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 12, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 12, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 12, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 13, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 13, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 13, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 13, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 13, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 14, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 14, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 14, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 14, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 14, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 15, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 15, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 15, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 15, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 15, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 16, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 16, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 16, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 16, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 16, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 17, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 17, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 17, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 17, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 17, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 18, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 18, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 18, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 18, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 18, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 19, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 19, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 19, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 19, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 19, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 20, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 20, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 20, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 20, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 21, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 21, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 21, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 21, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 21, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 22, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 22, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 22, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 22, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 22, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 23, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 23, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 23, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 23, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 23, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 24, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 24, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 24, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 24, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 24, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 25, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 25, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 25, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 25, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 25, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 26, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 26, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 26, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 26, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 26, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 27, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 27, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 27, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 27, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 27, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 28, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 28, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 28, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 28, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 28, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 29, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 29, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 29, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 29, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 29, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 30, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 30, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 30, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 30, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 30, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 31, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 31, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 31, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 31, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 31, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 32, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 32, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 32, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 32, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 32, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 33, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 33, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 33, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 33, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 33, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 34, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 34, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 34, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 34, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 34, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 35, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 35, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 35, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 35, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 35, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 36, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 36, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 36, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 36, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 36, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 37, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 37, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 37, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 37, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 37, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 38, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 38, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 38, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 38, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 38, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 39, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 39, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 39, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 39, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 40, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 40, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 40, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 40, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 40, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 41, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 41, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 41, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 41, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 41, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 42, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 42, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 42, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 42, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 42, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 43, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 43, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 43, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 43, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 43, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 44, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 44, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 44, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 44, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 44, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 45, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 45, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 45, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 45, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 45, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 46, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 46, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 46, CIFAR-10 Batch 3:  The loss was: 2.2999\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 46, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 46, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 47, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 47, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 47, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 47, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 47, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 48, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 48, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 48, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 48, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 48, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 49, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 49, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 49, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 49, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 49, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 50, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 50, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 50, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 50, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 50, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 51, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 51, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 51, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 51, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 51, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 52, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 52, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 52, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 52, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 52, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 53, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 53, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 53, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 53, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 53, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 54, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 54, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 54, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 54, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 54, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 55, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 55, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 55, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 55, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 55, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 56, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 56, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 56, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 56, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 56, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 57, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 57, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 57, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 57, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 57, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 58, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 58, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 58, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 58, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 59, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 59, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 59, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 59, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 59, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 60, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 60, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 60, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 60, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 60, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 61, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 61, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 61, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 61, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 61, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 62, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 62, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 62, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 62, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 62, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 63, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 63, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 63, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 63, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 63, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 64, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 64, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 64, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 64, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 64, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 65, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 65, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 65, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 65, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 65, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 66, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 66, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 66, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 66, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 66, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 67, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 67, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 67, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 67, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 67, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 68, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 68, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 68, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 68, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 68, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 69, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 69, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 69, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 69, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 69, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 70, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 70, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 70, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 70, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 70, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 71, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 71, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 71, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 71, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 71, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 72, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 72, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 72, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 72, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 72, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 73, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 73, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 73, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 73, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 73, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 74, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 74, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 74, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 74, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 74, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 75, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 75, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 75, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 75, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 75, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 76, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 76, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 76, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 76, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 76, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 77, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 77, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 77, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 77, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 78, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 78, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 78, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 78, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 78, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 79, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 79, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 79, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 79, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 79, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 80, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 80, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 80, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 80, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 80, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 81, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 81, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 81, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 81, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 81, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 82, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 82, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 82, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 82, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 82, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 83, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 83, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 83, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 83, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 83, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 84, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 84, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 84, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 84, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 84, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 85, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 85, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 85, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 85, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 85, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 86, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 86, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 86, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 86, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 86, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 87, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 87, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 87, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 87, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 87, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 88, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 88, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 88, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 88, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 88, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 89, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 89, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 89, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 89, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 89, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 90, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 90, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 90, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 90, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 90, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 91, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 91, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 91, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 91, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 91, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 92, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 92, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 92, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 92, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 92, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 93, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 93, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 93, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 93, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 93, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 94, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 94, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 94, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 94, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 94, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 95, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 95, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 95, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 95, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 95, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 96, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 96, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 96, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 96, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 96, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 97, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 97, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 97, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 97, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 98, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 98, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 98, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 98, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 98, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 99, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 99, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 99, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 99, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 99, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 100, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 100, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 100, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 100, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 100, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 101, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 101, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 101, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 101, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 101, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 102, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 102, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 102, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 102, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 102, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 103, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 103, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 103, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 103, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 103, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 104, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 104, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 104, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 104, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 104, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 105, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 105, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 105, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 105, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 105, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 106, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 106, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 106, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 106, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 106, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 107, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 107, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 107, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 107, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 107, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 108, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 108, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 108, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 108, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 108, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 109, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 109, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 109, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 109, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 109, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 110, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 110, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 110, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 110, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 110, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 111, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 111, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 111, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 111, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 111, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 112, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 112, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 112, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 112, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 112, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 113, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 113, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 113, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 113, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 113, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 114, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 114, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 114, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 114, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 114, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 115, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 115, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 115, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 115, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 115, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 116, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 116, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 116, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 116, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 117, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 117, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 117, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 117, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 117, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 118, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 118, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 118, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 118, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 118, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 119, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 119, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 119, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 119, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 119, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 120, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 120, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 120, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 120, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 120, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 121, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 121, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 121, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 121, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 121, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 122, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 122, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 122, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 122, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 122, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 123, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 123, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 123, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 123, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 123, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 124, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 124, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 124, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 124, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 124, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 125, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 125, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 125, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 125, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 125, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 126, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 126, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 126, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 126, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 126, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 127, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 127, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 127, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 127, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 127, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 128, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 128, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 128, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 128, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 128, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 129, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 129, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 129, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 129, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 129, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 130, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 130, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 130, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 130, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 130, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 131, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 131, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 131, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 131, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 131, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 132, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 132, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 132, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 132, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 132, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 133, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 133, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 133, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 133, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 133, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 134, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 134, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 134, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 134, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 134, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 135, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 135, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 135, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 135, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 135, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 136, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 136, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 136, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 136, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 136, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 137, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 137, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 137, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 137, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 137, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 138, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 138, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 138, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 138, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 138, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 139, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 139, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 139, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 139, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 139, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 140, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 140, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 140, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 140, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 140, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 141, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 141, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 141, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 141, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 141, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 142, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 142, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 142, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 142, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 142, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 143, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 143, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 143, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 143, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 143, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 144, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 144, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 144, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 144, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 144, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 145, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 145, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 145, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 145, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 145, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 146, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 146, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 146, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 146, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 146, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 147, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 147, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 147, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 147, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 147, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 148, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 148, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 148, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 148, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 148, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 149, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 149, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 149, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 149, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 149, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 150, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 150, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 150, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 150, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 150, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 151, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 151, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 151, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 151, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 151, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 152, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 152, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 152, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 152, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 152, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 153, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 153, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 153, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 153, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 153, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 154, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 154, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 154, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 154, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 155, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 155, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 155, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 155, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 155, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 156, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 156, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 156, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 156, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 156, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 157, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 157, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 157, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 157, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 157, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 158, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 158, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 158, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 158, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 158, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 159, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 159, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 159, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 159, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 159, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 160, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 160, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 160, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 160, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 160, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 161, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 161, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 161, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 161, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 161, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 162, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 162, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 162, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 162, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 162, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 163, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 163, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 163, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 163, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 163, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 164, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 164, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 164, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 164, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 164, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 165, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 165, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 165, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 165, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 165, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 166, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 166, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 166, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 166, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 166, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 167, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 167, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 167, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 167, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 167, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 168, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 168, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 168, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 168, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 168, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 169, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 169, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 169, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 169, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 169, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 170, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 170, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 170, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 170, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 170, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 171, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 171, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 171, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 171, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 171, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 172, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 172, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 172, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 172, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 172, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 173, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 173, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 173, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 173, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 173, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 174, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 174, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 174, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 174, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 174, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 175, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 175, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 175, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 175, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 175, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 176, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 176, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 176, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 176, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 176, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 177, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 177, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 177, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 177, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 177, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 178, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 178, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 178, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 178, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 178, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 179, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 179, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 179, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 179, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 179, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 180, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 180, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 180, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 180, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 180, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 181, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 181, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 181, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 181, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 181, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 182, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 182, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 182, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 182, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 182, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 183, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 183, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 183, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 183, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 183, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 184, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 184, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 184, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 184, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 184, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 185, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 185, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 185, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 185, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 185, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 186, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 186, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 186, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 186, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 186, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 187, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 187, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 187, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 187, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 187, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 188, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 188, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 188, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 188, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 188, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 189, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 189, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 189, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 189, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 189, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 190, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 190, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 190, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 190, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 190, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 191, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 191, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 191, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 191, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 191, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 192, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 192, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 192, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 192, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 192, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 193, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 193, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 193, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 193, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 193, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 194, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 194, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 194, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 194, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 194, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 195, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 195, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 195, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 195, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 195, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 196, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 196, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 196, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 196, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 196, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 197, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 197, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 197, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 197, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 197, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 198, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 198, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 198, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 198, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 198, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 199, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 199, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 199, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 199, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 199, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 200, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 200, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 200, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 200, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 200, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 201, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 201, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 201, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 201, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 201, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 202, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 202, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 202, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 202, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 202, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 203, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 203, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 203, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 203, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 203, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 204, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 204, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 204, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 204, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 204, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 205, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 205, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 205, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 205, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 205, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 206, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 206, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 206, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 206, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 206, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 207, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 207, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 207, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 207, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 207, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 208, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 208, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 208, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 208, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 208, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 209, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 209, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 209, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 209, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 209, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 210, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 210, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 210, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 210, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 210, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 211, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 211, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 211, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 211, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 211, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 212, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 212, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 212, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 212, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 212, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 213, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 213, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 213, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 213, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 213, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 214, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 214, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 214, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 214, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 214, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 215, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 215, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 215, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 215, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 215, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 216, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 216, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 216, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 216, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 216, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 217, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 217, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 217, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 217, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 217, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 218, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 218, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 218, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 218, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 218, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 219, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 219, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 219, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 219, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 219, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 220, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 220, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 220, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 220, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 220, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 221, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 221, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 221, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 221, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 221, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 222, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 222, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 222, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 222, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 222, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 223, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 223, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 223, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 223, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 223, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 224, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 224, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 224, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 224, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 224, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 225, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 225, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 225, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 225, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 225, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 226, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 226, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 226, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 226, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 226, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 227, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 227, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 227, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 227, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 227, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 228, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 228, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 228, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 228, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 228, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 229, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 229, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 229, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 229, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 229, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 230, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 230, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 230, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 230, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 230, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 231, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 231, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 231, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 231, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 231, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 232, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 232, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 232, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 232, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 232, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 233, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 233, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 233, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 233, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 233, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 234, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 234, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 234, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 234, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 234, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 235, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 235, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 235, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 235, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 235, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 236, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 236, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 236, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 236, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 236, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 237, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 237, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 237, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 237, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 237, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 238, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 238, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 238, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 238, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 238, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 239, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 239, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 239, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 239, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 239, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 240, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 240, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 240, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 240, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 240, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 241, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 241, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 241, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 241, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 241, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 242, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 242, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 242, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 242, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 242, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 243, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 243, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 243, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 243, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 243, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 244, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 244, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 244, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 244, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 244, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 245, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 245, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 245, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 245, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 245, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 246, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 246, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 246, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 246, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 246, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 247, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 247, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 247, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 247, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 247, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 248, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 248, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 248, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 248, CIFAR-10 Batch 4:  The loss was: 2.3007\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 248, CIFAR-10 Batch 5:  The loss was: 2.3023\n",
      "The validation accuracy was: 0.0942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 249, CIFAR-10 Batch 1:  The loss was: 2.3027\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 249, CIFAR-10 Batch 2:  The loss was: 2.3012\n",
      "The validation accuracy was: 0.0942\n",
      "Epoch 249, CIFAR-10 Batch 3:  The loss was: 2.3000\n",
      "The validation accuracy was: 0.0942\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./image_classification\n",
      "Testing Accuracy: 0.6892578125\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9c9fcd6898>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
